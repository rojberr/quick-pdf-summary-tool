{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e48f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install huggingface_hub PyPDF2 transformers nltk fpdf google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "705ec09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your huggingface token and input it here!\n",
    "# https://huggingface.co/settings/tokens\n",
    "import os\n",
    "os.environ['HF_TOKEN'] = 'HUGGINGFACE_TOKEN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08352a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "import PyPDF2\n",
    "\n",
    "with open('ai_future_jobs_report.pdf', 'rb') as file:\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceeca70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count (tokenize): 206779\n"
     ]
    }
   ],
   "source": [
    "# Check number of Tokens\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\") \n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Token count (tokenize):\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca64d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk count 226\n",
      "Max token count in a chunk: 1020\n",
      "Chunk 1: 989 tokens\n",
      "Chunk 2: 1004 tokens\n",
      "Chunk 3: 1016 tokens\n",
      "Chunk 4: 135 tokens\n",
      "Chunk 5: 1020 tokens\n",
      "Chunk 6: 491 tokens\n",
      "Chunk 7: 1009 tokens\n",
      "Chunk 8: 999 tokens\n",
      "Chunk 9: 997 tokens\n",
      "Chunk 10: 1000 tokens\n",
      "Chunk 11: 983 tokens\n",
      "Chunk 12: 997 tokens\n",
      "Chunk 13: 987 tokens\n",
      "Chunk 14: 1001 tokens\n",
      "Chunk 15: 854 tokens\n",
      "Chunk 16: 995 tokens\n",
      "Chunk 17: 870 tokens\n",
      "Chunk 18: 1013 tokens\n",
      "Chunk 19: 966 tokens\n",
      "Chunk 20: 1017 tokens\n",
      "Chunk 21: 675 tokens\n",
      "Chunk 22: 1013 tokens\n",
      "Chunk 23: 997 tokens\n",
      "Chunk 24: 995 tokens\n",
      "Chunk 25: 1000 tokens\n",
      "Chunk 26: 998 tokens\n",
      "Chunk 27: 987 tokens\n",
      "Chunk 28: 755 tokens\n",
      "Chunk 29: 806 tokens\n",
      "Chunk 30: 1003 tokens\n",
      "Chunk 31: 831 tokens\n",
      "Chunk 32: 810 tokens\n",
      "Chunk 33: 823 tokens\n",
      "Chunk 34: 625 tokens\n",
      "Chunk 35: 843 tokens\n",
      "Chunk 36: 677 tokens\n",
      "Chunk 37: 979 tokens\n",
      "Chunk 38: 1001 tokens\n",
      "Chunk 39: 1016 tokens\n",
      "Chunk 40: 1012 tokens\n",
      "Chunk 41: 988 tokens\n",
      "Chunk 42: 1015 tokens\n",
      "Chunk 43: 883 tokens\n",
      "Chunk 44: 977 tokens\n",
      "Chunk 45: 1011 tokens\n",
      "Chunk 46: 985 tokens\n",
      "Chunk 47: 1011 tokens\n",
      "Chunk 48: 826 tokens\n",
      "Chunk 49: 830 tokens\n",
      "Chunk 50: 842 tokens\n",
      "Chunk 51: 1005 tokens\n",
      "Chunk 52: 985 tokens\n",
      "Chunk 53: 1013 tokens\n",
      "Chunk 54: 1009 tokens\n",
      "Chunk 55: 1010 tokens\n",
      "Chunk 56: 1016 tokens\n",
      "Chunk 57: 1004 tokens\n",
      "Chunk 58: 816 tokens\n",
      "Chunk 59: 1005 tokens\n",
      "Chunk 60: 999 tokens\n",
      "Chunk 61: 996 tokens\n",
      "Chunk 62: 1008 tokens\n",
      "Chunk 63: 1019 tokens\n",
      "Chunk 64: 990 tokens\n",
      "Chunk 65: 1016 tokens\n",
      "Chunk 66: 999 tokens\n",
      "Chunk 67: 936 tokens\n",
      "Chunk 68: 893 tokens\n",
      "Chunk 69: 1000 tokens\n",
      "Chunk 70: 959 tokens\n",
      "Chunk 71: 1010 tokens\n",
      "Chunk 72: 700 tokens\n",
      "Chunk 73: 799 tokens\n",
      "Chunk 74: 708 tokens\n",
      "Chunk 75: 696 tokens\n",
      "Chunk 76: 986 tokens\n",
      "Chunk 77: 804 tokens\n",
      "Chunk 78: 999 tokens\n",
      "Chunk 79: 951 tokens\n",
      "Chunk 80: 907 tokens\n",
      "Chunk 81: 898 tokens\n",
      "Chunk 82: 1012 tokens\n",
      "Chunk 83: 993 tokens\n",
      "Chunk 84: 800 tokens\n",
      "Chunk 85: 991 tokens\n",
      "Chunk 86: 995 tokens\n",
      "Chunk 87: 940 tokens\n",
      "Chunk 88: 910 tokens\n",
      "Chunk 89: 992 tokens\n",
      "Chunk 90: 1014 tokens\n",
      "Chunk 91: 612 tokens\n",
      "Chunk 92: 1014 tokens\n",
      "Chunk 93: 576 tokens\n",
      "Chunk 94: 998 tokens\n",
      "Chunk 95: 1004 tokens\n",
      "Chunk 96: 994 tokens\n",
      "Chunk 97: 981 tokens\n",
      "Chunk 98: 838 tokens\n",
      "Chunk 99: 1008 tokens\n",
      "Chunk 100: 1006 tokens\n",
      "Chunk 101: 934 tokens\n",
      "Chunk 102: 979 tokens\n",
      "Chunk 103: 977 tokens\n",
      "Chunk 104: 704 tokens\n",
      "Chunk 105: 1020 tokens\n",
      "Chunk 106: 137 tokens\n",
      "Chunk 107: 968 tokens\n",
      "Chunk 108: 708 tokens\n",
      "Chunk 109: 705 tokens\n",
      "Chunk 110: 968 tokens\n",
      "Chunk 111: 998 tokens\n",
      "Chunk 112: 341 tokens\n",
      "Chunk 113: 999 tokens\n",
      "Chunk 114: 1012 tokens\n",
      "Chunk 115: 38 tokens\n",
      "Chunk 116: 1020 tokens\n",
      "Chunk 117: 358 tokens\n",
      "Chunk 118: 89 tokens\n",
      "Chunk 119: 1020 tokens\n",
      "Chunk 120: 179 tokens\n",
      "Chunk 121: 1006 tokens\n",
      "Chunk 122: 1012 tokens\n",
      "Chunk 123: 811 tokens\n",
      "Chunk 124: 757 tokens\n",
      "Chunk 125: 1000 tokens\n",
      "Chunk 126: 628 tokens\n",
      "Chunk 127: 998 tokens\n",
      "Chunk 128: 766 tokens\n",
      "Chunk 129: 991 tokens\n",
      "Chunk 130: 982 tokens\n",
      "Chunk 131: 1011 tokens\n",
      "Chunk 132: 991 tokens\n",
      "Chunk 133: 975 tokens\n",
      "Chunk 134: 462 tokens\n",
      "Chunk 135: 1018 tokens\n",
      "Chunk 136: 925 tokens\n",
      "Chunk 137: 1014 tokens\n",
      "Chunk 138: 1003 tokens\n",
      "Chunk 139: 1007 tokens\n",
      "Chunk 140: 866 tokens\n",
      "Chunk 141: 997 tokens\n",
      "Chunk 142: 1009 tokens\n",
      "Chunk 143: 959 tokens\n",
      "Chunk 144: 992 tokens\n",
      "Chunk 145: 944 tokens\n",
      "Chunk 146: 1009 tokens\n",
      "Chunk 147: 1015 tokens\n",
      "Chunk 148: 896 tokens\n",
      "Chunk 149: 788 tokens\n",
      "Chunk 150: 1003 tokens\n",
      "Chunk 151: 923 tokens\n",
      "Chunk 152: 978 tokens\n",
      "Chunk 153: 948 tokens\n",
      "Chunk 154: 1001 tokens\n",
      "Chunk 155: 1001 tokens\n",
      "Chunk 156: 1015 tokens\n",
      "Chunk 157: 964 tokens\n",
      "Chunk 158: 927 tokens\n",
      "Chunk 159: 907 tokens\n",
      "Chunk 160: 1005 tokens\n",
      "Chunk 161: 954 tokens\n",
      "Chunk 162: 1005 tokens\n",
      "Chunk 163: 1010 tokens\n",
      "Chunk 164: 723 tokens\n",
      "Chunk 165: 1005 tokens\n",
      "Chunk 166: 813 tokens\n",
      "Chunk 167: 998 tokens\n",
      "Chunk 168: 1001 tokens\n",
      "Chunk 169: 994 tokens\n",
      "Chunk 170: 922 tokens\n",
      "Chunk 171: 682 tokens\n",
      "Chunk 172: 1017 tokens\n",
      "Chunk 173: 969 tokens\n",
      "Chunk 174: 1012 tokens\n",
      "Chunk 175: 988 tokens\n",
      "Chunk 176: 932 tokens\n",
      "Chunk 177: 848 tokens\n",
      "Chunk 178: 999 tokens\n",
      "Chunk 179: 42 tokens\n",
      "Chunk 180: 1020 tokens\n",
      "Chunk 181: 178 tokens\n",
      "Chunk 182: 989 tokens\n",
      "Chunk 183: 1002 tokens\n",
      "Chunk 184: 1002 tokens\n",
      "Chunk 185: 950 tokens\n",
      "Chunk 186: 674 tokens\n",
      "Chunk 187: 1014 tokens\n",
      "Chunk 188: 952 tokens\n",
      "Chunk 189: 1009 tokens\n",
      "Chunk 190: 997 tokens\n",
      "Chunk 191: 1000 tokens\n",
      "Chunk 192: 1009 tokens\n",
      "Chunk 193: 1011 tokens\n",
      "Chunk 194: 1019 tokens\n",
      "Chunk 195: 1010 tokens\n",
      "Chunk 196: 991 tokens\n",
      "Chunk 197: 1004 tokens\n",
      "Chunk 198: 1002 tokens\n",
      "Chunk 199: 978 tokens\n",
      "Chunk 200: 1016 tokens\n",
      "Chunk 201: 1018 tokens\n",
      "Chunk 202: 1005 tokens\n",
      "Chunk 203: 1003 tokens\n",
      "Chunk 204: 969 tokens\n",
      "Chunk 205: 1017 tokens\n",
      "Chunk 206: 931 tokens\n",
      "Chunk 207: 989 tokens\n",
      "Chunk 208: 942 tokens\n",
      "Chunk 209: 1011 tokens\n",
      "Chunk 210: 1005 tokens\n",
      "Chunk 211: 827 tokens\n",
      "Chunk 212: 939 tokens\n",
      "Chunk 213: 1009 tokens\n",
      "Chunk 214: 892 tokens\n",
      "Chunk 215: 952 tokens\n",
      "Chunk 216: 1006 tokens\n",
      "Chunk 217: 1011 tokens\n",
      "Chunk 218: 996 tokens\n",
      "Chunk 219: 1010 tokens\n",
      "Chunk 220: 1008 tokens\n",
      "Chunk 221: 1004 tokens\n",
      "Chunk 222: 992 tokens\n",
      "Chunk 223: 1012 tokens\n",
      "Chunk 224: 1008 tokens\n",
      "Chunk 225: 962 tokens\n",
      "Chunk 226: 138 tokens\n"
     ]
    }
   ],
   "source": [
    "# Chunk it\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "max_words = 510\n",
    "max_tokens = 1020\n",
    "\n",
    "chunks = []\n",
    "current_chunk = []\n",
    "current_length = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized = tokenizer.tokenize(sentence)\n",
    "    sentence_length = len(tokenized)\n",
    "    \n",
    "    if sentence_length > max_tokens:\n",
    "        # Flush current_chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "        # Now split the long sentence\n",
    "        for i in range(0, sentence_length, max_tokens):\n",
    "            sub_tokens = tokenized[i:i+max_tokens]\n",
    "            sub_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(sub_tokens), skip_special_tokens=True)\n",
    "            chunks.append(sub_text)\n",
    "\n",
    "    elif current_length + sentence_length <= max_tokens:\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += sentence_length\n",
    "    else:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk = [sentence]\n",
    "        current_length = sentence_length\n",
    "\n",
    "if current_chunk:\n",
    "    chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "print(\"Chunk count\", len(chunks))\n",
    "\n",
    "max_tokens = 0\n",
    "for chunk in chunks:\n",
    "    if max_tokens < len(tokenizer.tokenize(chunk)):\n",
    "        max_tokens = len(tokenizer.tokenize(chunk))\n",
    "print(\"Max token count in a chunk:\", max_tokens)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    token_count = len(tokenizer.tokenize(chunk))\n",
    "    print(f\"Chunk {i+1}: {token_count} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9cf4cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 300, but your input_length is only 137. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=68)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przyklad streszczenia \n",
      " The 2025 Index is our most comprehensive to date and arrives at an important moment. New in this year’s report are in-depth analyses of the evolving landscape of AI hardware and novel estimates of inference costs. We also introduce fresh data on corporate adoption of responsible AI practices. The Index continues to lead in tracking and interpreting the most critical trends shaping the field. It has been cited in major media outlets such as The New York Times, Bloomberg, and The Guardian. It is referenced in hundreds of academic papers; and used by policymakers and government agencies around the world. We continue to serve as an independent source of insights for the global AI ecosystem. Explore the report and see for yourself this year's edition of the AI Index report. For more information on the report, visit the report's website or read it in its entirety here:  AI Index Report 2025Artificial Intelligence Index Report 20251, 20253, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2036, 2037, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065\n",
      "Reszta streszczenia \n",
      "\n",
      "The 2025 Index is our most comprehensive to date and arrives at an important moment. New in this year’s report are in-depth analyses of the evolving landscape of AI hardware and novel estimates of inference costs. We also introduce fresh data on corporate adoption of responsible AI practices. The Index continues to lead in tracking and interpreting the most critical trends shaping the field. It has been cited in major media outlets such as The New York Times, Bloomberg, and The Guardian. It is referenced in hundreds of academic papers; and used by policymakers and government agencies around the world. We continue to serve as an independent source of insights for the global AI ecosystem. Explore the report and see for yourself this year's edition of the AI Index report. For more information on the report, visit the report's website or read it in its entirety here:  AI Index Report 2025Artificial Intelligence Index Report 20251, 20253, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2036, 2037, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065 In 2024, U.S. private AI investment grew to $109.1 billion—nearly 12 times China’s $9.3 billion. Generative AI saw particularly strong momentum, attracting $33.9 billion globally in private investment. In 2024, global cooperation on AI governance intensified, with organizations including the OECD, EU, and African Union releasing frameworks on transparency, trustworthiness, and other core responsible AI principles. AI-related incidents are rising sharply, yet standardized RAI evaluations remain rare among major industrial model developers. But new benchmarks like HELM Safety, AIR-Bench, and FACTS offer promising tools for assessing factuality and safety. The global AI optimism is rising, but deep regional divides remain. In countries like China, Indonesia, and Thailand, strong majorities see AI products and services as more beneficial than harmful. In contrast, optimism remains far lower in places like Canada (40%), the United States (39%), and the Netherlands (36%). Still, sentiment is shifting: Since 2022, optimism has grown significantly in several previously skeptical countries, including Germany. Two-thirds of countries now offer or plan to offer K–12 CS education. Nearly 90% of notable AI models in 2024 came from industry, up from 60% in 2023. Complex reasoning remains a challenge for AI models. The AI Index welcomes feedback and new ideas for next year. The writing process was aided by AI tools, ChatGPT and Claude. The Index was conceived within the One Hundred Year Study on Artificial Intelligence (AI100). The AI index welcomes feedback, new ideas, and suggestions for the next year’s edition. The next edition of the AI Index will be released April 25, 2025. For more information, visit artificialintelligence index.org or follow us on Twitter @ArtificialIntelligenceIndex and @StanfordIntelligence. For the full report, go to artificialintelligence Index.org/Index/20255/The-Artificial-Intelligence-Index-25th-Annual-Report-20255. The full report will be available on April 26, 2025, at 10am ET. The AI Index 2025 Report is supplemented by raw data and an interactive tool. We invite each reader to use the data and the tool in a way most relevant to their work and interests. The Global AI Vibrancy tool will be updated in the summer of 2025. The public data and high-resolution images of all the charts in the report are available on Google Drive. The AI index 2025 Annual Report by Stanford University is licensed under Attribution-NoDerivatives 4.0 International. For confidential support call the Samaritans in the UK on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Line on 1-800-273-TALK (8255). For confidential help in the United States, call theNational Suicide Prevention Lifeline at 1-844-788-8255 or visit http://www.suicidepreventionlifeline.org/. The AI Index is an independent initiative at the Stanford Institute for Human-Centered Artificial Intelligence (HAI) The AI Index would like to acknowledge the following contributions of data, advice, and expert commentary included in the AI Index Report 2025: contributions of individuals, advice by chapter and section, and analysis of data. The report is available in English, Spanish, Portuguese, and Arabic. The full text of the report can be found at the bottom of the page. For help with reading the report, contact the authors at loredana.fattorini, yolanda. Gil, and Ray Perrault, or call 1-800-273-8255. For confidential support, call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, or see www.samaritans.org. In the U.S. call the National Suicide Prevention Lifeline at 1- 800- 273-7255 or visit http://www.suicidepreventionlifeline.org/.\n"
     ]
    }
   ],
   "source": [
    "# Summarize the PDF\n",
    "# Use BART\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "# torch.mps.empty_cache() # for Apple Mx Silicon\n",
    "torch.cuda.empty_cache() # for NVIDIA GPUs, non-CUDA systems\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", framework=\"pt\")\n",
    "\n",
    "summary_output = summarizer(chunks[0], max_length=300, min_length=100, do_sample=False)\n",
    "summary_text = summary_output[0]['summary_text']\n",
    "\n",
    "def extract_summary(x):\n",
    "    result = summarizer(x, max_length=300, min_length=200, do_sample=False)\n",
    "    if result[0]['summary_text']:\n",
    "        return result[0]['summary_text']\n",
    "    else:\n",
    "        \" \"\n",
    "\n",
    "chunks_summaries = list(map(extract_summary, chunks[0:5]))\n",
    "\n",
    "print(\"Przyklad streszczenia \\n\", chunks_summaries[0])\n",
    "\n",
    "print(\"Reszta streszczenia \\n\")\n",
    "print(*chunks_summaries, sep = \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61badcc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use Gemini if you have API_KEY\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sleep\n\u001b[32m      5\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mPlease translate the following text to German. Return only the translated text without any explanations or additional comments: \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# Use Gemini if you have API_KEY\n",
    "from google import genai\n",
    "from time import sleep\n",
    "\n",
    "prompt = \"Please translate the following text to German. Return only the translated text without any explanations or additional comments: \"\n",
    "\n",
    "translations = []\n",
    "client = genai.Client(api_key=\"API_KEY\")\n",
    "for i, chunk_summary in enumerate(chunks_summaries):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt + chunk_summary,\n",
    "    )\n",
    "    translations.append(response.text)\n",
    "    sleep(3)\n",
    "\n",
    "print(F\"Translation\\n: {translations[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local LMStudio model\n",
    "# Run LMStudio model\n",
    "# TODO use LMStudio SDK\n",
    "import requests\n",
    "\n",
    "def translate_text(text):\n",
    "    url = \"http://localhost:1234/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"bielik-7b-instruct-v0.1\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a translation assistant. Translate the given English text to Polish. I want you to give me only translated text, do not add anything else.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{text}\" }\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": -1,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    result = response.json()\n",
    "\n",
    "    translated_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    # model adds <s> token at the beginning of the translated text so we need too remove it\n",
    "    translated_text = translated_text.replace(\"<s>\", \"\").strip()\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "bielik_translations = []\n",
    "for chunk_summary in chunks_summaries:\n",
    "    bielik_translations.append(translate_text(chunk_summary))\n",
    "\n",
    "print(\"Example translation: \\n\", bielik_translations[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c15d3fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create PDF with translated summary\n",
    "from fpdf import FPDF\n",
    "\n",
    "pdf = FPDF()\n",
    "pdf.set_auto_page_break(auto=True, margin=15)\n",
    "pdf.add_page()\n",
    "\n",
    "pdf.add_font(\"DejaVu\", \"\", \"DejaVuSans.ttf\", uni=True)\n",
    "pdf.set_font(\"DejaVu\", size=12)\n",
    "\n",
    "# for i, chunk in enumerate(bielik_translations):\n",
    "for i, chunk in enumerate(chunks_summaries):\n",
    "    pdf.multi_cell(0, 10, f\"\\n{chunk}\")\n",
    "    pdf.ln()\n",
    "\n",
    "pdf.output(\"article_summary.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
